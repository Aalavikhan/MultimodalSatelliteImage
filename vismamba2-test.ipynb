{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Mamba Energy Forecasting\n",
    "\n",
    "**Architecture**: Vision Mamba + Temporal Features + Multimodal Fusion\n",
    "\n",
    "**Goal**: WAPE < 10%, MAPE < 10% on test set\n",
    "\n",
    "**Key Features**:\n",
    "- Handles missing satellite images gracefully (zero tensors as fallback)\n",
    "- Robust data type handling for mixed numpy/pandas types\n",
    "- Time series split: Train â‰¤2020, Val 2021-2022, Test >2022\n",
    "- 21 engineered features including lags, rolling stats, and growth rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "# Paths\n",
    "DATA_CSV = r'/kaggle/input/satimage2/data.csv'\n",
    "IMAGE_DIR = r'/kaggle/input/satimage2/images/images'\n",
    "WORKSPACE = Path(r'/kaggle/working')\n",
    "\n",
    "assert Path(DATA_CSV).exists(), 'CSV not found'\n",
    "assert Path(IMAGE_DIR).exists(), 'Image directory not found'\n",
    "\n",
    "print(f'CSV: {DATA_CSV}')\n",
    "print(f'Images: {IMAGE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "df.columns = ['Country', 'date', 'demand_twh', 'area_sqkm', 'Population', 'per_capita_kwh']\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m')\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df = df.sort_values(['Country', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Create complete date range for each country\n",
    "countries = df['Country'].unique()\n",
    "date_range = pd.date_range(df['date'].min(), df['date'].max(), freq='MS')\n",
    "\n",
    "complete_df = []\n",
    "for country in countries:\n",
    "    country_df = pd.DataFrame({\n",
    "        'Country': country,\n",
    "        'date': date_range\n",
    "    })\n",
    "    complete_df.append(country_df)\n",
    "\n",
    "complete_df = pd.concat(complete_df, ignore_index=True)\n",
    "df = complete_df.merge(df, on=['Country', 'date'], how='left')\n",
    "\n",
    "# Forward fill within each country\n",
    "for col in ['demand_twh', 'area_sqkm', 'Population', 'per_capita_kwh']:\n",
    "    df[col] = df.groupby('Country')[col].fillna(method='ffill')\n",
    "    df[col] = df.groupby('Country')[col].fillna(method='bfill')\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Date range: {df[\"date\"].min()} to {df[\"date\"].max()}')\n",
    "print(f'Countries: {len(countries)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index available images\n",
    "image_index = {}\n",
    "for country_dir in Path(IMAGE_DIR).iterdir():\n",
    "    if country_dir.is_dir():\n",
    "        country = country_dir.name\n",
    "        for img_file in country_dir.glob('*.tif'):\n",
    "            parts = img_file.stem.split('_')\n",
    "            if len(parts) == 3:\n",
    "                try:\n",
    "                    year, month = int(parts[1]), int(parts[2])\n",
    "                    image_index[(country, year, month)] = str(img_file)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "print(f'Indexed {len(image_index)} images')\n",
    "\n",
    "# Add image availability flag\n",
    "df['has_image'] = df.apply(lambda row: (row['Country'], row['year'], row['month']) in image_index, axis=1)\n",
    "print(f'Image coverage: {df[\"has_image\"].mean()*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns first\n",
    "FEATURE_COLS = [\n",
    "    'log_demand', 'log_population', 'log_area', 'log_per_capita', 'log_density',\n",
    "    'month_sin', 'month_cos', 'year_normalized',\n",
    "    'lag_1', 'lag_2', 'lag_3', 'lag_6', 'lag_12',\n",
    "    'rolling_mean_3', 'rolling_mean_6', 'rolling_mean_12',\n",
    "    'rolling_std_3', 'rolling_std_6', 'rolling_std_12',\n",
    "    'demand_growth', 'population_growth'\n",
    "]\n",
    "\n",
    "# Feature engineering\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure numeric types for base columns\n",
    "    numeric_cols = ['demand_twh', 'Population', 'area_sqkm', 'per_capita_kwh']\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Log transforms\n",
    "    df['log_demand'] = np.log1p(df['demand_twh'].astype(float))\n",
    "    df['log_population'] = np.log1p(df['Population'].astype(float))\n",
    "    df['log_area'] = np.log1p(df['area_sqkm'].astype(float))\n",
    "    df['log_per_capita'] = np.log1p(df['per_capita_kwh'].astype(float))\n",
    "    \n",
    "    # Density\n",
    "    df['density'] = df['Population'].astype(float) / (df['area_sqkm'].astype(float) + 1)\n",
    "    df['log_density'] = np.log1p(df['density'])\n",
    "    \n",
    "    # Time features\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'].astype(float) / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'].astype(float) / 12)\n",
    "    year_min = df['year'].min()\n",
    "    year_max = df['year'].max()\n",
    "    df['year_normalized'] = (df['year'].astype(float) - year_min) / (year_max - year_min + 1e-8)\n",
    "    \n",
    "    # Lags and rolling features per country\n",
    "    for lag in [1, 2, 3, 6, 12]:\n",
    "        df[f'lag_{lag}'] = df.groupby('Country')['demand_twh'].shift(lag).astype(float)\n",
    "    \n",
    "    for window in [3, 6, 12]:\n",
    "        df[f'rolling_mean_{window}'] = df.groupby('Country')['demand_twh'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        ).astype(float)\n",
    "        df[f'rolling_std_{window}'] = df.groupby('Country')['demand_twh'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).std()\n",
    "        ).astype(float)\n",
    "    \n",
    "    # Growth rates\n",
    "    df['demand_growth'] = df.groupby('Country')['demand_twh'].pct_change().astype(float)\n",
    "    df['population_growth'] = df.groupby('Country')['Population'].pct_change().astype(float)\n",
    "    \n",
    "    # Fill NaNs and ensure float type\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # Ensure all feature columns are float64\n",
    "    for col in FEATURE_COLS:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_features(df)\n",
    "print(f'Features created. Shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split by year\n",
    "train_df = df[df['year'] <= 2020].copy()\n",
    "val_df = df[(df['year'] > 2020) & (df['year'] <= 2022)].copy()\n",
    "test_df = df[df['year'] > 2022].copy()\n",
    "\n",
    "print(f'Train: {len(train_df)} ({train_df[\"year\"].min()}-{train_df[\"year\"].max()})')\n",
    "print(f'Val: {len(val_df)} ({val_df[\"year\"].min()}-{val_df[\"year\"].max()})')\n",
    "print(f'Test: {len(test_df)} ({test_df[\"year\"].min()}-{test_df[\"year\"].max()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "import cv2\n",
    "import rasterio\n",
    "\n",
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, df, image_index, image_size=128):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_index = image_index\n",
    "        self.image_size = image_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def load_image(self, country, year, month):\n",
    "        key = (country, year, month)\n",
    "        if key not in self.image_index:\n",
    "            return np.zeros((self.image_size, self.image_size), dtype=np.float32)\n",
    "        \n",
    "        try:\n",
    "            with rasterio.open(self.image_index[key]) as src:\n",
    "                img = src.read(1).astype(np.float32)\n",
    "            \n",
    "            # Handle NaN and inf\n",
    "            img = np.nan_to_num(img, nan=0, posinf=0, neginf=0)\n",
    "            \n",
    "            # Resize\n",
    "            if img.shape != (self.image_size, self.image_size):\n",
    "                img = cv2.resize(img, (self.image_size, self.image_size))\n",
    "            \n",
    "            # Normalize\n",
    "            p1, p99 = np.percentile(img[img > 0], [1, 99]) if (img > 0).any() else (0, 1)\n",
    "            img = np.clip((img - p1) / (p99 - p1 + 1e-8), 0, 1)\n",
    "            \n",
    "            return img\n",
    "        except:\n",
    "            return np.zeros((self.image_size, self.image_size), dtype=np.float32)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img = self.load_image(row['Country'], row['year'], row['month'])\n",
    "        img = torch.from_numpy(img).unsqueeze(0).float()  # [1, H, W] - ensure float32\n",
    "        \n",
    "        # Features - convert to numpy array explicitly to handle mixed types\n",
    "        feature_values = []\n",
    "        for col in FEATURE_COLS:\n",
    "            val = row[col]\n",
    "            # Convert to float, handling any non-numeric types\n",
    "            if isinstance(val, (int, float, np.integer, np.floating)):\n",
    "                feature_values.append(float(val))\n",
    "            else:\n",
    "                feature_values.append(0.0)\n",
    "        features = torch.tensor(feature_values, dtype=torch.float32)\n",
    "        \n",
    "        # Target - explicit conversion\n",
    "        target_val = float(row['demand_twh']) if pd.notna(row['demand_twh']) else 0.0\n",
    "        target = torch.tensor([target_val], dtype=torch.float32)\n",
    "        \n",
    "        # Image availability\n",
    "        has_img = torch.tensor([1.0 if row['has_image'] else 0.0], dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            'image': img,\n",
    "            'features': features,\n",
    "            'target': target,\n",
    "            'has_image': has_img\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EnergyDataset(train_df, image_index)\n",
    "val_dataset = EnergyDataset(val_df, image_index)\n",
    "test_dataset = EnergyDataset(test_df, image_index)\n",
    "\n",
    "print(f'Train samples: {len(train_dataset)}')\n",
    "print(f'Val samples: {len(val_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "\n",
    "# Test loading\n",
    "sample = train_dataset[0]\n",
    "print(f'\\nSample shapes:')\n",
    "print(f'  Image: {sample[\"image\"].shape}')\n",
    "print(f'  Features: {sample[\"features\"].shape}')\n",
    "print(f'  Target: {sample[\"target\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Mamba implementation\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=128, patch_size=16, in_chans=1, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # 3 stride-2 convs: 128 -> 64 -> 32 -> 16\n",
    "        final_size = img_size // 8  # 128 / 8 = 16\n",
    "        self.n_patches = final_size * final_size  # 16 * 16 = 256\n",
    "        \n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, embed_dim // 4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, embed_dim, H', W']\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        return x\n",
    "\n",
    "class SSMBlock(nn.Module):\n",
    "    \"\"\"Selective State Space Block.\"\"\"\n",
    "    def __init__(self, dim, state_dim=16):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "        # Projections\n",
    "        self.in_proj = nn.Linear(dim, dim * 2)\n",
    "        self.conv1d = nn.Conv1d(dim, dim, kernel_size=3, padding=1, groups=dim)\n",
    "        \n",
    "        # SSM parameters\n",
    "        self.x_proj = nn.Linear(dim, state_dim)\n",
    "        self.dt_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        # State transition\n",
    "        self.A = nn.Parameter(torch.randn(dim, state_dim))\n",
    "        self.B = nn.Parameter(torch.randn(dim, state_dim))\n",
    "        self.C = nn.Parameter(torch.randn(dim, state_dim))\n",
    "        self.D = nn.Parameter(torch.ones(dim))\n",
    "        \n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        residual = x\n",
    "        \n",
    "        # Split for gating\n",
    "        x_and_gate = self.in_proj(self.norm(x))\n",
    "        x, gate = x_and_gate.chunk(2, dim=-1)\n",
    "        \n",
    "        # Conv1d processing\n",
    "        x = rearrange(x, 'b l d -> b d l')\n",
    "        x = self.conv1d(x)\n",
    "        x = rearrange(x, 'b d l -> b l d')\n",
    "        x = F.silu(x)\n",
    "        \n",
    "        # Simplified SSM - state space computation\n",
    "        state = self.x_proj(x)  # [B, L, state_dim]\n",
    "        A = F.softplus(self.A)  # [D, state_dim]\n",
    "        \n",
    "        # Compute state transition: project state back to D dimensions\n",
    "        # state: [B, L, state_dim], C: [D, state_dim]\n",
    "        # We want [B, L, D] output\n",
    "        y = torch.einsum('bls,ds->bld', state, self.C) + x * self.D.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Gating\n",
    "        y = y * F.silu(gate)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.out_proj(y)\n",
    "        \n",
    "        return residual + y\n",
    "\n",
    "class VisionMamba(nn.Module):\n",
    "    def __init__(self, img_size=128, patch_size=16, embed_dim=256, depth=6, num_classes=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans=1, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(0.1)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            SSMBlock(embed_dim) for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        \n",
    "        return x\n",
    "\n",
    "print('Vision Mamba model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete forecasting model\n",
    "class EnergyForecaster(nn.Module):\n",
    "    def __init__(self, n_features=len(FEATURE_COLS), embed_dim=256, img_size=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Vision encoder\n",
    "        self.vision_encoder = VisionMamba(img_size=img_size, embed_dim=embed_dim, depth=6)\n",
    "        \n",
    "        # Feature encoder\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Fusion\n",
    "        self.fusion = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True)\n",
    "        self.fusion_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Predictor\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, image, features, has_image):\n",
    "        # Encode vision\n",
    "        vis_emb = self.vision_encoder(image)\n",
    "        vis_emb = vis_emb * has_image  # Mask out missing images\n",
    "        \n",
    "        # Encode features\n",
    "        feat_emb = self.feature_encoder(features)\n",
    "        \n",
    "        # Fuse with attention\n",
    "        tokens = torch.stack([vis_emb, feat_emb], dim=1)  # [B, 2, D]\n",
    "        fused, _ = self.fusion(tokens, tokens, tokens)\n",
    "        fused = self.fusion_norm(fused + tokens)\n",
    "        fused = fused.mean(dim=1)  # [B, D]\n",
    "        \n",
    "        # Predict\n",
    "        out = self.predictor(fused)\n",
    "        return out\n",
    "\n",
    "model = EnergyForecaster().to(DEVICE)\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mae = np.abs(y_true - y_pred).mean()\n",
    "    mape = (np.abs((y_true - y_pred) / (y_true + 1e-8)).mean()) * 100\n",
    "    wape = (np.abs(y_true - y_pred).sum() / (np.abs(y_true).sum() + 1e-8)) * 100\n",
    "    rmse = np.sqrt(((y_true - y_pred) ** 2).mean())\n",
    "    return {'mae': mae, 'mape': mape, 'wape': wape, 'rmse': rmse}\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, scaler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        image = batch['image'].to(DEVICE)\n",
    "        features = batch['features'].to(DEVICE)\n",
    "        target = batch['target'].to(DEVICE)\n",
    "        has_image = batch['has_image'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            pred = model(image, features, has_image)\n",
    "            loss = criterion(pred, target)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            image = batch['image'].to(DEVICE)\n",
    "            features = batch['features'].to(DEVICE)\n",
    "            target = batch['target'].to(DEVICE)\n",
    "            has_image = batch['has_image'].to(DEVICE)\n",
    "            \n",
    "            pred = model(image, features, has_image)\n",
    "            \n",
    "            preds.append(pred.cpu().numpy())\n",
    "            targets.append(target.cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "    \n",
    "    metrics = compute_metrics(targets, preds)\n",
    "    return metrics, preds, targets\n",
    "\n",
    "print('Training utilities defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1000\n",
    "LR = 1e-4\n",
    "PATIENCE = 50\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=1)\n",
    "criterion = nn.HuberLoss(delta=1.0)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "print(f'Batch size: {BATCH_SIZE}')\n",
    "print(f'Learning rate: {LR}')\n",
    "print(f'Epochs: {EPOCHS}')\n",
    "print(f'Patience: {PATIENCE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_wape = float('inf')\n",
    "patience_counter = 0\n",
    "history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler)\n",
    "    \n",
    "    # Validate\n",
    "    val_metrics, _, _ = evaluate(model, val_loader)\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Track history\n",
    "    history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'val_mae': val_metrics['mae'],\n",
    "        'val_mape': val_metrics['mape'],\n",
    "        'val_wape': val_metrics['wape'],\n",
    "        'val_rmse': val_metrics['rmse']\n",
    "    })\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {train_loss:.4f} | Val WAPE: {val_metrics['wape']:.2f}% | Val MAPE: {val_metrics['mape']:.2f}% | MAE: {val_metrics['mae']:.4f}\")\n",
    "    \n",
    "    # Early stopping and checkpointing\n",
    "    if val_metrics['wape'] < best_val_wape:\n",
    "        best_val_wape = val_metrics['wape']\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_wape': best_val_wape,\n",
    "            'val_metrics': val_metrics\n",
    "        }, WORKSPACE / 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "print(f'\\nBest validation WAPE: {best_val_wape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "checkpoint = torch.load(WORKSPACE / 'best_model.pt', weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print('Validation metrics (best checkpoint):')\n",
    "for k, v in checkpoint['val_metrics'].items():\n",
    "    print(f'  {k.upper()}: {v:.4f}')\n",
    "\n",
    "print('\\nTest set evaluation:')\n",
    "test_metrics, test_preds, test_targets = evaluate(model, test_loader)\n",
    "for k, v in test_metrics.items():\n",
    "    print(f'  {k.upper()}: {v:.4f}')\n",
    "\n",
    "# Save predictions\n",
    "test_results = test_df[['Country', 'date', 'demand_twh']].copy()\n",
    "test_results['predicted'] = test_preds\n",
    "test_results['error'] = test_results['demand_twh'] - test_results['predicted']\n",
    "test_results['abs_error'] = np.abs(test_results['error'])\n",
    "test_results['pct_error'] = (test_results['abs_error'] / (test_results['demand_twh'] + 1e-8)) * 100\n",
    "test_results.to_csv(WORKSPACE / 'test_predictions.csv', index=False)\n",
    "print(f'\\nTest predictions saved to {WORKSPACE / \"test_predictions.csv\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training history\n",
    "hist_df = pd.DataFrame(history)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].plot(hist_df['epoch'], hist_df['train_loss'], label='Train Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(hist_df['epoch'], hist_df['val_wape'], label='WAPE', color='red')\n",
    "axes[0, 1].plot(hist_df['epoch'], hist_df['val_mape'], label='MAPE', color='blue')\n",
    "axes[0, 1].axhline(y=10, color='green', linestyle='--', label='10% Target')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Error %')\n",
    "axes[0, 1].set_title('Validation Metrics')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].scatter(test_targets, test_preds, alpha=0.5, s=10)\n",
    "axes[1, 0].plot([test_targets.min(), test_targets.max()], \n",
    "                [test_targets.min(), test_targets.max()], 'r--', lw=2)\n",
    "axes[1, 0].set_xlabel('Actual')\n",
    "axes[1, 0].set_ylabel('Predicted')\n",
    "axes[1, 0].set_title(f'Test Set Predictions (WAPE: {test_metrics[\"wape\"]:.2f}%)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "errors = test_targets.flatten() - test_preds.flatten()\n",
    "axes[1, 1].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Prediction Error')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Error Distribution')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(WORKSPACE / 'training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Results saved to {WORKSPACE / \"training_results.png\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hist_df = pd.DataFrame(history)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Train loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist_df['epoch'], hist_df['train_loss'], label='Train Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation metrics (WAPE and MAPE)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist_df['epoch'], hist_df['val_wape'], label='Val WAPE', color='red', linewidth=2)\n",
    "plt.plot(hist_df['epoch'], hist_df['val_mape'], label='Val MAPE', color='blue', linewidth=2)\n",
    "plt.axhline(y=10, color='green', linestyle='--', linewidth=1.5, label='10% Target')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Error %', fontsize=12)\n",
    "plt.title('Validation Metrics', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(WORKSPACE / 'training_validation_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Training curves saved to {WORKSPACE / \"training_validation_curves.png\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-country analysis\n",
    "print('Per-country test performance:')\n",
    "print('-' * 60)\n",
    "\n",
    "country_metrics = []\n",
    "for country in test_results['Country'].unique():\n",
    "    country_data = test_results[test_results['Country'] == country]\n",
    "    metrics = compute_metrics(country_data['demand_twh'].values, country_data['predicted'].values)\n",
    "    country_metrics.append({\n",
    "        'Country': country,\n",
    "        'MAE': metrics['mae'],\n",
    "        'MAPE': metrics['mape'],\n",
    "        'WAPE': metrics['wape'],\n",
    "        'Samples': len(country_data)\n",
    "    })\n",
    "\n",
    "country_df = pd.DataFrame(country_metrics).sort_values('WAPE')\n",
    "\n",
    "print('\\nTop 10 best performing countries:')\n",
    "print(country_df.head(10).to_string(index=False))\n",
    "\n",
    "print('\\nTop 10 worst performing countries:')\n",
    "print(country_df.tail(10).to_string(index=False))\n",
    "\n",
    "country_df.to_csv(WORKSPACE / 'country_performance.csv', index=False)\n",
    "print(f'\\nCountry-level results saved to {WORKSPACE / \"country_performance.csv\"}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7075094,
     "sourceId": 11312402,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
